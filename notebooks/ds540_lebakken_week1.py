# -*- coding: utf-8 -*-
"""DS540_Lebakken_Week1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/126UI1UTPz5M3BCdy61Lsc8vXnmxy_ZG1

# Set Up
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.chdir('/content/gdrive/MyDrive/DS_540/Project')

!ls

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
import sqlite3
import re

"""# Loading / EDA"""

# Loading Data
sample_data = pd.read_csv('nyc311_sample_2025_PROPORTIONAL_200k.csv')

print(sample_data.shape)
print(sample_data.columns)

sample_data.head()

# Finding greatest and least amount of complaint types
complaint_counts = sample_data['complaint_type'].value_counts()
print(f'Total Number of Complaint Categories: {len(complaint_counts)}')
print('=' * 50)
print('Highest Complaint Counts')
print('=' * 50)
print(complaint_counts.head(49))

print('=' * 50)
print('Lowest Complaint Counts')
print('=' * 50)
print(complaint_counts.tail(49))

print(complaint_counts)

"""# Preprocessing

## Dropping columns
"""

# Look at documentation for reasons why dropped
cols_to_drop = ['x_coordinate_state_plane', 'y_coordinate_state_plane', 'due_date', 'bbl', 'status', 'location', 'park_borough',
                'taxi_company_borough']
sample_data.drop(columns = cols_to_drop, inplace=True)

"""## Switch times to timestamps

"""

# Columns to be changed
timestamp_cols = ['created_date', 'closed_date', 'resolution_action_updated_date']

# Changing to timestamps
for col in timestamp_cols:
  sample_data[col] = pd.to_datetime(sample_data[col])

# Checking conversion
sample_data.info()

"""## Mixed Data Types"""

# Columns with mixed data types
mixed_dtype_cols = sample_data.columns[sample_data.apply(lambda x: x.apply(type).nunique() > 1)]

# Finding the unique data types for each column
for col in mixed_dtype_cols:
    print(f'Column: {col}')
    print(sample_data[col].apply(type).value_counts())

# Finding the amount of NaN values for mixed columns
for col in mixed_dtype_cols:
  print(f'Column: {col}')
  print(sample_data[col].isna().sum())

# Change mixed column NaNs to pd.NA
for col in mixed_dtype_cols:
  sample_data[col] = sample_data[col].fillna(pd.NA)

# Seeing if there are any mixed columns left
check = sample_data.columns[sample_data.apply(lambda x: x.apply(type).nunique() > 1)]
print(check)

"""## Numeric conversions"""

# Changing floats to ints


float_to_int_cols = ['council_district']

# Change NaN values to -99
for col in float_to_int_cols:
  sample_data[col] = sample_data[col].fillna(-99)

# Change from float to int
for col in float_to_int_cols:
  sample_data[col] = sample_data[col].astype(int)

# Filling NaN values in lat and long to 0.00
sample_data['latitude'] = sample_data['latitude'].fillna(0.00)
sample_data['longitude'] = sample_data['longitude'].fillna(0.00)

"""## Datatype conversion"""

# Changing objects to strings
object_cols = sample_data.columns[sample_data.dtypes == object]
sample_data[object_cols] = sample_data[object_cols].astype('string')

sample_data.info()

"""## Standardizing Text Fields"""

# Remove leading, trailing, and more than one space between text in string columns
# Converting all text to lowercase
for col in sample_data.columns[sample_data.dtypes == 'string']:

  sample_data[col] = (
      sample_data[col]
      .str.strip() # Trailing & leading
      .str.replace(r'\s+', ' ', regex = True) # Replacing multiple spaces with one
      .str.lower() # Converting all text to lowercase
      .str.replace(r'[^\w\s]', '', regex = True) # Removing punctuation
  )

"""# Creating new fields"""

# Create resolution time
sample_data['resolution_time_hours'] = (
    (sample_data['closed_date'] - sample_data['created_date'])
    .dt.total_seconds() / 3600
)

# Removing negative values
sample_data.loc[
    sample_data['resolution_time_hours'] < 0,
    'resolution_time_hours'
] = np.nan

# Log transform
sample_data['log_resolution_time_hours'] = np.log1p(sample_data['resolution_time_hours'])

# Create search_text
exclude_cols = ['open_data_channel_type'] # Excluded because of how call came in

text_cols = [
    col for col in sample_data.select_dtypes('string').columns
    if col not in exclude_cols
]

sample_data['search_text'] = (
    sample_data[text_cols]
    .fillna('') # Replacing na with ''
    .agg(' '.join, axis = 1) # Joining text across columns
    .str.replace(r'\s+', ' ', regex = True) # Removing extra spaces
    .str.strip()
)

# Print instances of search_text in full
print(sample_data['search_text'].iloc[0])
print(sample_data['search_text'].iloc[1000])

print(sample_data.columns)

"""# Complaint table"""

# Complaints table
complaints = sample_data.loc[:, [
    'unique_key',
    'created_date',
    'resolution_action_updated_date',
    'closed_date',
    'complaint_type',
    'descriptor',
    'descriptor_2',
    'incident_address',
    'city',
    'incident_zip',
    'borough',
    'search_text'
]]

complaints.head()

"""# Load Data into Database and Create Indexes for SQLite"""

# Convert timestamps to ISO strings for SQLite
datetime_cols = ['created_date', 'resolution_action_updated_date', 'closed_date']

for col in datetime_cols:
  complaints[col] = complaints[col].apply(lambda x: x.isoformat()
  if pd.notna(x) else None)

# Coverting pd.DA to None for SQLite
complaints = complaints.astype(object)
complaints = complaints.where(pd.notna(complaints), None)

# Connect to SQLite
connection = sqlite3.connect('nyc_311_complaints.db', timeout = 60)
cursor = connection.cursor()

# Creating SQL table
create_table_sql = """
CREATE TABLE IF NOT EXISTS complaints (
    unique_key INTEGER PRIMARY KEY,
    created_date TEXT,
    resolution_action_updated_date TEXT,
    closed_date TEXT,
    complaint_type TEXT,
    descriptor TEXT,
    descriptor_2 TEXT,
    incident_address TEXT,
    city TEXT,
    incident_zip TEXT,
    borough TEXT,
    search_text TEXT
    );
"""
cursor.execute(create_table_sql)
connection.commit()

# Inserting data in chunks of 5000
# Improves memory

insert_sql = """
INSERT OR REPLACE INTO complaints (
    unique_key,
    created_date,
    resolution_action_updated_date,
    closed_date,
    complaint_type,
    descriptor,
    descriptor_2,
    incident_address,
    city,
    incident_zip,
    borough,
    search_text
    )
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
"""

batch = 5000
for i in range (0, len(complaints), batch):
  end = i + batch
  chunk = complaints.iloc[i:end]

  cursor.executemany(insert_sql, chunk.values.tolist())

  print(f'Inserted rows {i} to {end}')

  connection.commit()

# Creating indexes with safeguards against duplicates
cursor.execute("CREATE INDEX IF NOT EXISTS idx_borough ON complaints (borough);")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_complaint_type ON complaints (complaint_type);")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_created_date ON complaints (created_date);")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_search_text ON complaints (search_text);")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_city ON complaints (city);")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_incident_zip ON complaints (incident_zip);")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_borough_created_date ON complaints (borough, created_date);")

connection.commit()

# FTS5 extension for full-text search for search_text

cursor.execute("""
CREATE VIRTUAL TABLE IF NOT EXISTS search_index USING fts5(
    unique_key,
    search_text
    );
""")
connection.commit()

# Add data to FTS5 table
fts_rows = complaints[['unique_key', 'search_text']].to_records(index = False).tolist()
cursor.executemany("INSERT INTO search_index (unique_key, search_text) VALUES (?, ?);", fts_rows)
connection.commit()

"""# Performance checks"""

# Keyword filter
keywords = 'music'

query_keywords = """
SELECT c.*
FROM complaints c
JOIN search_index fts
  ON c.unique_key = fts.unique_key
WHERE fts.search_text MATCH ?
LIMIT 10;
"""

df_keywords = pd.read_sql_query(query_keywords, connection, params=(keywords,))
print(f'=== Keyword: {keywords} search results ===')
print(df_keywords.head())

# Borough filter
borough_filter = 'brooklyn'

query_borough = """
SELECT *
FROM complaints
WHERE city = ? OR borough = ?
LIMIT 10;
"""

df_borough = pd.read_sql_query(query_borough, connection, params=(borough_filter, borough_filter))
print('\n=== Borough filter results ===')
print(df_borough.head())

# Date range filter
start_date = '2025-01-01'
end_date = '2025-01-31'

query_date = """
SELECT *
FROM complaints
WHERE created_date BETWEEN ? AND ?
LIMIT 10;
"""

df_date = pd.read_sql_query(query_date, connection, params=(start_date, end_date))
print('\n=== Date range filter results ===')
print(df_date.head())